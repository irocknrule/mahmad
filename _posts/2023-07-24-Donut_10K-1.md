---
title: "OCR-Free Document Understanding with Donut"
date: 2023-07-29T15:34:30-04:00
categories:
  - Blog
  - computer vision
tags:
  - cv
  - GenAI
  - Object Detection
  - OCR
classes: wide
---

Visual Document Understanding (VDU) is a field with a wealth of research for which I had a use-case come up at work recently. There was a rather repetitive (and laborious) task of reading through a long PDF report with the same format for each customer, selecting some specific and pre-determined values from the PDF and copying them over to an excel spreadsheet. This would take 15-30 minutes of my time and my first thought was trying to automate the steps using LayoutParser and some OCR engine, but I found an easier solution with the DOcumeNt Uderstanding Transformer (DONUT) model [1]. The paper is very interesting and I plan on publishing a full review of the paper soon, but before that I want to write about my experiments with this OCR free approach. 

For this post, I will use some publicly available financial reports from the Securities and Exchange Commision (SEC) for the company Tesla, specifically the form 10-K which is a mandatory annual report that all publicly traded companies in the US are required to file. It provides a comprehensive summary of a company's financial performance and business activities. We will consider only a few select pages in this experiment. Full reports are available within the SEC's EDGAR database [2].

## Donut Overview

As mentioned in the previous section, I will post a full review of the Donut paper in an upcoming post but as a short intro to Donut we can describe it as a sequence-to-sequence multimodal model which has a Swin Transformer as a vision encoder and BART as a text decoder. Donut understands documents by first encoding the visual features of the document with the Swin Transformer, which is a powerful image encoder which learns the spatial relationships between different parts of an image. These encoded visual features are then passed to a BART decoder trained on a large text corpus. The decoder then uses the encoded visual features to generate a text representation of the document. 

Donut can be used for visual document classification, visual question answering and information extraction. In this post we will look at the latter two specifically, starting off with question answering and then information extraction. Donut is really powerful and I was very impressed with how easy it is to get it off and running, especially since its a part of the Huggingface transformers library [3].

## Question answering

### Cover page

#### Model setup and inference

We first consider the cover page of the 10-K as shown below. 

{% include figure image_path="/assets/images/blogs/tesla_cover_2.jpg" alt="" caption="Cover page of sample 10-K downloaded from SEC archives."%}

We initialize and download the `docvqa` fine tuned model first from the model library:

```python
from transformers import DonutProcessor, VisionEncoderDecoderModel

processor = DonutProcessor.from_pretrained("naver-clova-ix/donut-base-finetuned-docvqa")
model = VisionEncoderDecoderModel.from_pretrained("naver-clova-ix/donut-base-finetuned-docvqa")
```

We then type out the set of questions we want to ask Donut to answer from the image. It is suggested to make the questions as specific as possible, because in this use case we are asking the model direct questions whose answers will readily be present in the image. The advantage of using a standard format document is that the answers to the questions will always be within the same template. Lets formulate the following questions:

```python
questions = ["what is the trading symbol?",
            "what is the name of each exchange where it is registered?",
            "is the registrant registrant is not required to file reports pursuant to Section 13 or 15(d) of the Act?",
            "is the registrant is a well-known seasoned issuer, as defined in Rule 405 of the Securities Act.",
            "what is the address of principal executive offices?",
            "what is the registrant's telephone number?",
            "what is the commision file number?",
            "what is the exact name of the registrant?",
            ]
```

We now create the prompt to send to the model in the specific format needed and then loop through each question to generate the exact prompt for Donut. 

```python
task_prompt = "<s_docvqa><s_question>{user_input}</s_question><s_answer>"
for each in questions:
  question = each
	prompt = task_prompt.replace("{user_input}", question)
	decoder_input_ids = processor.tokenizer(prompt,add_special_tokens=False, return_tensors="pt")["input_ids"]
	outputs = model.generate(pixel_values.to(device),
                  decoder_input_ids=decoder_input_ids.to(device),
                  max_length=model.decoder.config.max_position_embeddings,
	          early_stopping=True,
	          pad_token_id=processor.tokenizer.pad_token_id,
	          eos_token_id=processor.tokenizer.eos_token_id,
	          use_cache=True,
	          num_beams=1,
	          bad_words_ids=[[processor.tokenizer.unk_token_id]],
	          return_dict_in_generate=True,
	          output_scores=True)
	
seq = processor.batch_decode(outputs.sequences)[0]
print(processor.token2json(seq))
```

The processor then tokenizes the input prompt strings to convert them in to a Pytorch tensor `decoder_input_ids`. The pre-trained language model is now used to generate the response by passing this tokenized input to the `generate` function along with various generation options such as maximum length, early stopping etc. The decoder then decodes the generated output from the language model back into human readable format using the `batch_decode` method before the tokens are converted to JSON and printed out. 

#### Generated answers
Running the above code gives us the following output:

```
{'question': 'what is the trading symbol?', 'answer': 'tsla'}
{'question': 'what is the name of each exchange where it is registered?', 'answer': 'the nasdaq global select market'}
{'question': 'is the registrant registrant is not required to file reports pursuant to Section 13 or 15(d) of the Act?', 'answer': 'no'}
{'question': 'is the registrant is a well-known seasoned issuer, as defined in Rule 405 of the Securities Act.', 'answer': 'yes'}
{'question': 'what is the address of principal executive offices?', 'answer': '(512) 516-8177'}
{'question': "what is the registrant's telephone number?", 'answer': '(512) 516-8177'}
{'question': 'what is the commision file number?', 'answer': '001-34756'}
{'question': 'what is the exact name of the registrant?', 'answer': '(ir.s. employer)'}
```

Lets take a look at the responses in some detail. 

Q: What is the trading symbol? 
A: TSLA

Q: what is the name of each exchange where it is registered?
A: the nasdaq global select market

The extracted answers here are correct. The interesting point here is that answers were correctly extract from the line below the question wording in the table. The model did not do a left to right question to answer identification, but the correct top to bottom. Very cool and exciting to see this in action. 

Q: Is the registrant registrant is not required to file reports pursuant to Section 13 or 15(d) of the Act?
Answer: no

Q: is the registrant is a well-known seasoned issuer, as defined in Rule 405 of the Securities Act
A: yes

These answers were extracted from the check-boxes. Again both are correct as the model correctly correlated the right check box value with the question. Also, i had asked the questions in reverse order of the information in the image and the model had no problem at all. 

Q: what is the address of principal executive offices?, 
A: (512) 516-8177

Q: what is the registrant's telephone number?
A: (512) 516-8177

This the first point where the model trips up. The address box is not learnt correctly, as the next line (the phone number) is returned as the address. If we specifically ask the telephone number, the model replies correctly. The answers here are above the words linked to questions, unlike the table where the headers represented the question tokens. 

Q:  what is the commision file number?
A: 001-34756

No trouble here at all, the answer is to the right of the question identifiers and the model extracts it easily. 

Q: what is the exact name of the registrant?
A: (ir.s. employer)

The model inference here is completely incorrect. The name of the registrant is in a larger font above the contextual text but the model returns a value further away from the image. 

From these results, we see that the Donut model is working very efficiently overall out of the box for a rather complex image thrown at it and to me it is quite impressive. There are some issues and errors, which to me is actually better because a 100% accuracy would have indicated something weird and incorrect to me. 

## Overall thoughts

This post is just the first one where I am investigating and presenting the effectiveness of the Donut model for visual document understanding. It looks to exhibit extremely high performance out of the box and quite easy to implement without all the issues with traditional OCR engines. In a few follow up posts, I will present some more results of using Donut to infer information from tables, paragraphs and other pages of text along with combining them with Huggingface pipelines to carry out text summarization and other features of LLMs. 

If you would like to check them out before I get to actually post them in the upcoming days, here is the link to the full notebook. 


(financial_images/10K-extraction-donut.ipynb)[https://github.com/irocknrule/projects/blob/main/financial_images/10K-extraction-donut.ipynb]


## References

[1] OCR-free Document Understanding Transformer: https://arxiv.org/abs/2111.15664

[2] US Securities and Exchange commision, EDGAR: https://www.sec.gov/edgar/search-and-access

[3] https://huggingface.co/docs/transformers/model_doc/donut
