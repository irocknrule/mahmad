---
title: "OCR-Free Table and Text Reading with Donut"
date: 2023-07-31T15:34:30-04:00
categories:
  - Blog
  - computer vision
tags:
  - cv
  - GenAI
  - Object Detection
  - OCR
classes: wide
---

In my previous post, I talked about using the Document Understanding Transformer (DONUT) model to carry out Visual Document Understanding (VDU) and carry out question answering from some text with a specific format (i.e. a 10-K financial filing). In this post I describe more insightful results from my experimentation with the DONUT model as it leaves me highly impressed with its capabilities right out off the shelf. 

Here we look at how Donut extracts text from both a multi-column table followed by a wall-of-text, so it can be used as a standard OCR engine with no external packages. For an overview of the structure of Donut, check back on my previous post [here](https://irocknrule.github.io/mahmad/blog/computer%20vision/Donut_10K-1/).

## Question answering from tables

From the same 10-K filing document we used the last time for Tesla, we extract the page which contains the overall 'Result of Operations', a table with revenue details for the last 3 years, year over year differences along with breakdowns per sector. The full image is below:

{% include figure image_path="/assets/images/blogs/tesla_revenues.jpg" alt="" caption="Overall revenues table from Tesla 10-K."%}

Similar to the previous `docvqa` model, we have to initialize the appropriate Donut processor and model:

```python

from transformers import DonutProcessor, VisionEncoderDecoderModel

processor = DonutProcessor.from_pretrained("naver-clova-ix/donut-base-finetuned-docvqa")
model = VisionEncoderDecoderModel.from_pretrained("naver-clova-ix/donut-base-finetuned-docvqa")

```

We now set up using the GPU for inference (if present) and setup the questions to ask Donut followed by looping through each question and passing the appropriate prompt to the model for inference. 

```python
device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)

questions = ["what is the total revenues in 2021?",
			       "what is the total automotive revenues in 2019?",
             "what is the 2021 vs 2020 % change for energy generation?"
            ]
```

```python
task_prompt = "<s_docvqa><s_question>{user_input}</s_question><s_answer>"
for each in questions:
	question = each
	prompt = task_prompt.replace("{user_input}", question)
	decoder_input_ids = processor.tokenizer(prompt, add_special_tokens=False, return_tensors="pt")["input_ids"]
	outputs = model.generate(pixel_values.to(device),	
	                        decoder_input_ids=decoder_input_ids.to(device),
                        	max_length=model.decoder.config.max_position_embeddings,
                        	early_stopping=True,
                        	pad_token_id=processor.tokenizer.pad_token_id,
                        	eos_token_id=processor.tokenizer.eos_token_id,
                        	use_cache=True,
                        	num_beams=1,
                        	bad_words_ids=[[processor.tokenizer.unk_token_id]],
                        	return_dict_in_generate=True,
                        	output_scores=True)

seq = processor.batch_decode(outputs.sequences)[0]
print(processor.token2json(seq))
```

## Generated Answers

Running the code above, we get the following output:

```python
{'question': 'what is the total revenues in 2021?', 'answer': '$ 53,823'}
{'question': 'what is the total automotive revenues in 2019?', 'answer': '20,821'} 
{'question': 'what is the 2021 vs 2020 % change for energy generation?', 'answer': '40%'}
```

Each answer above is correct. Donut successfully parses multiple columns and rows in the table and correlates the (row,column) box to generate the correct answer each time. For example, the first question is asking for the Total Revenue in 2021. This would mean looking at the last row in column 1 and the next column to get the total revenue value, which is $53, 823. Similarly for question 2, it takes a row from the middle of the table and the column under the 2019 year for revenues to select 20,821. Note that it did not select the next column value, which is also a dollar amount. Finally it correctly selected the percent change value for the third question too. 

This level of accuracy from reading a table without OCR is frankly amazing in my opinion. OCR is generally so hard to set up as the accuracy is dependent on the engine plus proper layouts needs a whole lot of training from annotated data. Donut makes things so much easier here. 

## OCR from text page

We saw already how well Donut is performing to answer questions from documents without OCR, but what if we simply require reading text and not answer questions from the image? Essentially, can Donut do the job of an OCR reader and if yes, how well? My last experiment was to do just this operation with Donut. 

I took the following page with a paragraph of text (in italics) and wanted to use Donut to read the text directly. 

{% include figure image_path="/assets/images/blogs/statement.jpg" alt="" caption="Wall of text including a forward-looking statement."%}

To simply read the text (similar to OCR) we need to use the `synthdog` task name in the prompt to the Donut Model. In the paper, the authors provide more details on how they generated synthetic data to pre-train their images and they describe the usage of their synthetic document generator. 

We use the new `finetuned-cord-v2` model for this exercise:

```python
task_name = "synthdog"
task_prompt = f"<s_{task_name}>"

processor = DonutProcessor.from_pretrained("naver-clova-ix/donut-base-finetuned-cord-v2")
model = VisionEncoderDecoderModel.from_pretrained("naver-clova-ix/donut-base-finetuned-cord-v2")
```

The other steps are similar to the previous ones of sending the inputs to the model for inference and then parsing the output before printing it. 

```python
task_prompt = task_prompt
decoder_input_ids = processor.tokenizer(task_prompt, add_special_tokens=False, return_tensors="pt")["input_ids"]

device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)


outputs = model.generate(pixel_values.to(device),
                        decoder_input_ids=decoder_input_ids.to(device),
                        max_length=model.decoder.config.max_position_embeddings,
                        early_stopping=True,
                        pad_token_id=processor.tokenizer.pad_token_id,
                        eos_token_id=processor.tokenizer.eos_token_id,
                        use_cache=True,
                        num_beams=1,
                        bad_words_ids=[[processor.tokenizer.unk_token_id]],
                        return_dict_in_generate=True,
                        output_scores=True,)

sequence = processor.batch_decode(outputs.sequences)[0]
sequence = sequence.replace(processor.tokenizer.eos_token, "").replace(processor.tokenizer.pad_token, "")
sequence = re.sub(r"<.*?>", "", sequence, count=1).strip() # remove first task start token
print(sequence)
```

The output from this step:

```
<s_menu><s_nm> Forward-Looking Statements The discussions in this Annual Report on Form 10-K contain forward-looking statements reflecting our current expectations that involve risks and uncertainties. These forward-looking statements include. but are not limited to. statements concerning any potential future impact of the coronavirus disease ("COVID-19") pandemic on our business supply chain constraints, our strategic; competition, future operations and production capacty, future financial poston, future revenues, projected costs, profitability, expected to restrictions capital adequacy, expectations regarding demand academic for our technologies, growth opportunities and trends in the market in which we operate, prospers and plans and objectives of management. The words "anticipates" "believes," includes "expects, "tends" may's plots, projects." "well." would" and similarly statements, although not allow-looking statements contain these experiences are intended to identify forward-looking statements, although not allow-looking statements identifiying words. We may not actually achieve the plans, intentions or expectations disclosed in our forward-looking statements identified by responds not actually sometimes actually restrics or events could different material/from and you're should not place made relincc on our forward-looking statements. Actival results or event include Thes forward-looking statements the plans intentions and expectations disclosed in the forward-looking statements that we make. Thes forward-looking statements, involve risks and uncertainties that could cause our actual results to different materially from those in the forward-looking statements, including, without limitation, thatrisks set forth in Part I. Item LA. "Risk factors" in this Annual Report on Form 10-K and in our other flipes with the Securities and Exchange Commission (the "SEC"). We do not assume any obligation to update any forward-looking statements. statements.
```

We can see that Donut has extracted the full wall of text quite easily with not many errors. There are some small issues (the paragraph ends with the word 'statements' twice, a couple words were not read correctly like 'position' as read as 'poston') but overall the text is definitely readable. This text can now be used easily for any other function, for eg. sending it through a BERT/LLM model to summarize, ask questions and so on. I tried it with the HuggingFace pipelines and the results are again pretty interesting. That will be in another upcoming post. 

## Finishing Up

This second post detailing my experimentation with Donut illustrates how useful this model is and I can think of so many applications where it can be used to make life easier for analysts and developers working with PDFs and images to read data. Pretty much any application using OCR can benefit from Donut. Again, the link to the full notebook with all these experiments are available on my github below:

[financial_images/10K-extraction-donut.ipynb](https://github.com/irocknrule/projects/blob/main/financial_images/10K-extraction-donut.ipynb)

For more information on using Donut and fine-tuning your own models, check out these really well written notebooks here by *NielsRogge* [https://github.com/NielsRogge/Transformers-Tutorials/tree/master/Donut](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/Donut). I used bits and pieces of the code from these notebooks in the above experiments, so full credit goes to the author above. 

